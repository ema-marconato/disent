{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from disent.dataset import DisentDataset\n",
    "from disent.dataset.data import XYObjectData, Shapes3dData, SpritesData\n",
    "from disent.dataset.transform import ToImgTensorF32\n",
    "from disent.frameworks.vae import BetaVae\n",
    "from disent.model import AutoEncoder\n",
    "from disent.model.ae import DecoderConv64\n",
    "from disent.model.ae import EncoderConv64\n",
    "from disent.util import is_test_run  # you can ignore and remove this\n",
    "\n",
    "# prepare the data\n",
    "data = Shapes3dData(prepare=False)\n",
    "data = SpritesData(prepare=True)\n",
    "\n",
    "dataset = DisentDataset(data, transform=ToImgTensorF32())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=10)\n",
    "\n",
    "# create the pytorch lightning system\n",
    "module: L.LightningModule = BetaVae(\n",
    "    model=AutoEncoder(\n",
    "        encoder=EncoderConv64(x_shape=data.x_shape, z_size=8, z_multiplier=2),\n",
    "        decoder=DecoderConv64(x_shape=data.x_shape, z_size=8),\n",
    "    ),\n",
    "    cfg=BetaVae.cfg(optimizer=\"adam\", optimizer_kwargs=dict(lr=1e-3), loss_reduction=\"mean_sum\", beta=4),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3\n",
      "[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:55326 (errno: 98 - Address already in use).\n",
      "/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3\n",
      "/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3\n",
      "[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:55326 (errno: 98 - Address already in use).\n",
      "[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 147, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py\", line 570, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py\", line 933, in _run\n    self.strategy.setup_environment()\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py\", line 143, in setup_environment\n    self.setup_distributed()\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py\", line 192, in setup_distributed\n    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/fabric/utilities/distributed.py\", line 246, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 900, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 245, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 176, in _create_c10d_store\n    return TCPStore(\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:55326 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:55326 (errno: 98 - Address already in use).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(logger\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fast_dev_run\u001b[39m=\u001b[39mis_test_run(), max_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, )\n\u001b[1;32m      3\u001b[0m                     \u001b[39m# enable_checkpointing=True, default_root_dir='.data/checkpoints')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(module, dataloader)\n",
      "File \u001b[0;32m/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    529\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 531\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    533\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:41\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mlauncher\u001b[39m.\u001b[39;49mlaunch(trainer_fn, \u001b[39m*\u001b[39;49margs, trainer\u001b[39m=\u001b[39;49mtrainer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py:124\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m process_context \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mstart_processes(\n\u001b[1;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapping_function,\n\u001b[1;32m    118\u001b[0m     args\u001b[39m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     join\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  \u001b[39m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocs \u001b[39m=\u001b[39m process_context\u001b[39m.\u001b[39mprocesses\n\u001b[0;32m--> 124\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m process_context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    125\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    127\u001b[0m worker_output \u001b[39m=\u001b[39m return_queue\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 147, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py\", line 570, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py\", line 933, in _run\n    self.strategy.setup_environment()\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py\", line 143, in setup_environment\n    self.setup_distributed()\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/pytorch/strategies/ddp.py\", line 192, in setup_distributed\n    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/lightning/fabric/utilities/distributed.py\", line 246, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 900, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 245, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/mnt/data/emanuele.marconato/miniconda3/envs/disent-py38/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 176, in _create_c10d_store\n    return TCPStore(\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:55326 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:55326 (errno: 98 - Address already in use).\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer = L.Trainer(logger=True, fast_dev_run=is_test_run(), max_epochs=3, )\n",
    "                    # enable_checkpointing=True, default_root_dir='.data/checkpoints')\n",
    "trainer.fit(module, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dci.informativeness_train': 0.96025,\n",
       " 'dci.informativeness_test': 0.24649999999999997,\n",
       " 'dci.disentanglement': 0.02376654453393539,\n",
       " 'dci.completeness': 0.02985560412256319,\n",
       " 'mig.discrete_score': 0.035976604417420434}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from disent.metrics import metric_dci, metric_mig\n",
    "from disent.util import is_test_run\n",
    "\n",
    "get_repr = lambda x: module.encode(x.to(module.device))\n",
    "\n",
    "# evaluate\n",
    "{\n",
    "    **metric_dci(\n",
    "        dataset,\n",
    "        get_repr,\n",
    "        num_train=10 if is_test_run() else 1000,\n",
    "        num_test=5 if is_test_run() else 500,\n",
    "        boost_mode=\"sklearn\",\n",
    "    ),\n",
    "    **metric_mig(dataset, get_repr, num_train=20 if is_test_run() else 2000),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
